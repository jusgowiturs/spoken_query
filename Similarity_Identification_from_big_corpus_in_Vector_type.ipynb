{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BHUDJBqreF19"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pickle\n",
        "\n",
        "class Similarity:\n",
        "  def __init__(self):\n",
        "\n",
        "    self.model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose a different model based on your needs\n",
        "    self.Data_Empty = pd.DataFrame([])#columns= [\"ID\",\"Q0\",\"Doc_Name\",\"Score\"])\n",
        "  def load_dataset(self, dataset_path):\n",
        "    self.dataset = np.load(dataset_path)\n",
        "    print(self.dataset.shape)\n",
        "\n",
        "  def find_similar_documents(self,top_k):\n",
        "\n",
        "    #print(f\"\\nIn Find Similarmethod {self.query} is executing...\\n\")\n",
        "    query_embedding = self.model.encode(self.query,show_progress_bar = True,convert_to_tensor= True)\n",
        "\n",
        "    #print(query_embedding, self.dataset[:,:-1].shape)\n",
        "    query_embedding = query_embedding.reshape(1, -1)\n",
        "    #print(f\"\\n Shapeof Query Embedding{query_embedding.shape}\")\n",
        "    self.cosine_scores = cosine_similarity(query_embedding, self.dataset[:,:-1]).flatten()\n",
        "\n",
        "    self.top_results = np.argsort(self.cosine_scores)[-top_k:][::-1]\n",
        "    #self.cosine_scores = self.cosine_scores.sort()[-top_k:][::-1]\n",
        "    self.cosine_scores = self.cosine_scores[self.top_results]\n",
        "    #print(f\"\\n\\nTop results \\t:\\t{self.top_results}\")\n",
        "    #print(f\"\\n\\nMaximum index as top results \\t:\\t{self.top_results[0].max()}\")\n",
        "    return self.top_results[:top_k],self.cosine_scores[:top_k]\n",
        "  def process(self,dataset_path:str,query,top_k:int,filename:str):\n",
        "    self.load_dataset(dataset_path)\n",
        "    self.LabelDecoder()\n",
        "    q_text = query['Text'].tolist()\n",
        "    q_Id   = query[\"ID\"].tolist()\n",
        "    #q_text = query[\"Text\n",
        "    #print(q_text)\n",
        "\n",
        "    for q in tqdm(range(len(q_text))):\n",
        "      self.query = q_text[q]\n",
        "      tmp_q = [q_Id[q][3:]] * top_k   # Query ID is en_140 ID should be 140\n",
        "      #print(tmp_q)\n",
        "      tmp_q0 = ['Q0']*top_k\n",
        "      self.top,self.score = self.find_similar_documents(top_k)\n",
        "      #print(type(self.top))\n",
        "      self.top = [self.dataset[i,-1] for i in self.top]\n",
        "      self.top = [self.label[int(i)] for i in self.top]\n",
        "      Rank =  range(1,top_k+1)\n",
        "      self.score = np.round(self.score,2)\n",
        "      #self.top = pd.DataFrame(self.top,columns = ['Doc_Name'])\n",
        "      #self.score = pd.DataFrame(self.score,columns = ['Score'])\n",
        "      self.DataQ_Rel_Frame = pd.DataFrame([tmp_q,tmp_q0,self.top,Rank,self.score]).T\n",
        "      self.Data_Empty = pd.concat([self.DataQ_Rel_Frame,self.Data_Empty],axis = 0,ignore_index=True)\n",
        "      print(self.Data_Empty.shape)\n",
        "    self.Data_Empty.columns = ['ID','Q0','Doc_Name','Rank','Score']\n",
        "    self.Data_Empty.to_csv(PATH+filename)\n",
        "  # def _process(self, QID,DOCNAME,SCORE):\n",
        "\n",
        "  #   return pd.DataFrame([QID,DOCNAME,SCORE])\n",
        "\n",
        "  def LabelDecoder(self):\n",
        "    self.label = pickle.load(open(PATH+'VectorDB_1000/LabelDecoder1.pkl','rb'))\n",
        "    return self.label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "j_pAwHAMeOja",
        "outputId": "d7e2fbc0-1dd6-4ff7-ccec-0f8d104ffc79"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sentence_transformers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-84c645bcc940>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msentence_transformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentenceTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "71pWK_bBeVjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Q-Relative"
      ],
      "metadata": {
        "id": "SpkFCiI_eV_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH            = \"/content/drive/MyDrive/SQCLIR_IITM/Final_en_corpus/\"\n",
        "dataset_path    = PATH+'VectorDB_1000/Dataset_447543_385.npy'\n",
        "Query_similar   = Similarity()\n",
        "#Query_similar.load_dataset(dataset_path)\n",
        "#top,score       = Query_similar.find_similar_documents(top_k = 1000)\n",
        "Query_similar.process(dataset_path,en_wave_query_train,999,filename = \"Predicted_Q_rel_Train_14_10.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "l1P_-qhjeSuI",
        "outputId": "222d941c-d031-4f34-8645-0b78a034094d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Similarity' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f295b5ad351f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mPATH\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/SQCLIR_IITM/Final_en_corpus/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset_path\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'VectorDB_1000/Dataset_447543_385.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mQuery_similar\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mSimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#Query_similar.load_dataset(dataset_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#top,score       = Query_similar.find_similar_documents(top_k = 1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Similarity' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o2p6GhKbeef8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test query"
      ],
      "metadata": {
        "id": "8TCFh0v-egc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH            = \"/content/drive/MyDrive/SQCLIR_IITM/Final_en_corpus/\"\n",
        "dataset_path    = PATH+'VectorDB_1000/Dataset_447543_385.npy'\n",
        "Query_similar   = Similarity()\n",
        "#Query_similar.load_dataset(dataset_path)\n",
        "#top,score       = Query_similar.find_similar_documents(top_k = 1000)\n",
        "Query_similar.process(dataset_path,en_wave_query_test,1000,filename = \"Predicted_Q_rel_Test_14_10_1000.csv\")\n",
        "import pandas as pd\n",
        "Test = pd.read_csv(PATH+\"Predicted_Q_rel_Test_14_10_1000.csv\")\n",
        "Test.head()\n",
        "Test['Run_ID'] =['en-IITM_BS-AEKD-run1']*Test.shape[0]\n",
        "Test.head()\n",
        "Test.drop(['Unnamed: 0'],axis = 1,inplace = True)\n",
        "# Step 2: Write the DataFrame to a TXT file\n",
        "txt_file_path = PATH+'en-IITM_MiniLM_1000.txt'  # Replace with your desired TXT file path\n",
        "Test.to_csv(txt_file_path, sep='\\t', index=False, header=True)  # Use tab as a separator\n",
        "#\n",
        "Test.head()"
      ],
      "metadata": {
        "id": "TIiO_NUbekEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jPLY739MfP8W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}